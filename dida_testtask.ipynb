{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmZX9J9j5Wf1Lf0ONJshNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/celaira/roof_deep_learning/blob/main/dida_testtask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Images:**"
      ],
      "metadata": {
        "id": "PPh7PSi0Q1NQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mount google drive:"
      ],
      "metadata": {
        "id": "rJMyJrHot4J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XnQnolw4tvrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d333c01a-78fb-4e62-f4e1-56db16d94851"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import stuff"
      ],
      "metadata": {
        "id": "OPGsrPc69ld0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dZ8y2_wSttKD"
      },
      "outputs": [],
      "source": [
        "#import json\n",
        "import os\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, UpSampling2D, InputLayer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create path to data:"
      ],
      "metadata": {
        "id": "MB-ZFURR9pPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_image_path = '/content/drive/MyDrive/training_image'\n",
        "label_path = '/content/drive/MyDrive/label'\n",
        "test_image_path = '/content/drive/MyDrive/test_image'"
      ],
      "metadata": {
        "id": "UCKzdCwE9xJJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import the images:"
      ],
      "metadata": {
        "id": "ma3w6Zp_NDnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    filenames = sorted(os.listdir(folder))\n",
        "    for filename in filenames:\n",
        "        img = cv.imread(os.path.join(folder, filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return np.array(images), filenames\n",
        "\n",
        "train_images, train_image_filenames = load_images_from_folder(training_image_path)\n",
        "labels, label_filenames = load_images_from_folder(label_path)\n",
        "test_images, test_image_filenames = load_images_from_folder(test_image_path)\n",
        "\n",
        "# Ensure the training images and labels are aligned by sorting the filenames\n",
        "train_image_filenames_sorted = sorted(train_image_filenames)\n",
        "label_filenames_sorted = sorted(label_filenames)\n",
        "\n",
        "aligned_train_images = [cv.imread(os.path.join(training_image_path, filename)) for filename in train_image_filenames_sorted]\n",
        "aligned_labels = [cv.imread(os.path.join(label_path, filename), cv.IMREAD_GRAYSCALE) for filename in label_filenames_sorted]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "aligned_train_images = np.array(aligned_train_images)\n",
        "aligned_labels = np.array(aligned_labels)\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "# Normalize the images\n",
        "aligned_train_images = aligned_train_images / 255.0\n",
        "aligned_labels = aligned_labels / 255.0\n",
        "test_images = test_images / 255.0\n"
      ],
      "metadata": {
        "id": "KumvKEJMNDzw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating validation for better error-testing:"
      ],
      "metadata": {
        "id": "qD_6Yh2w-TkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize the labels\n",
        "aligned_labels = np.where(aligned_labels > 0.5, 1, 0)\n",
        "\n",
        "# Add an extra dimension to the data\n",
        "aligned_train_images = aligned_train_images[..., np.newaxis]\n",
        "aligned_labels = aligned_labels[..., np.newaxis]\n",
        "test_images = test_images[..., np.newaxis]\n",
        "\n",
        "# Split training and validation data\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(aligned_train_images, aligned_labels, test_size=0.2, random_state=42)\n",
        "print(aligned_train_images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX7W7HgkA5ZX",
        "outputId": "fd048b11-c990-4f5a-b457-29f8bb7de277"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 256, 256, 3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define a UNET-Model:**\n",
        "\n",
        "unet_model is a function that takes input_shape as an argument, which defines the shape of the input images (e.g., height, width, and number of channels).\n",
        "\n",
        "layers.Input creates a Keras tensor, which will act as the input layer of the model. The shape of this input is defined by input_shape."
      ],
      "metadata": {
        "id": "5POvkJOJAuUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the U-Net model\n",
        "def unet_model(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    # Bottleneck\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    # Decoder\n",
        "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = layers.concatenate([u6, c4])\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "\n",
        "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = layers.concatenate([u7, c3])\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "\n",
        "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = layers.concatenate([u8, c2])\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "\n",
        "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = layers.concatenate([u9, c1])\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "\n",
        "    outputs = layers.Conv2D(2, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model"
      ],
      "metadata": {
        "id": "-62pJlhUA0hX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Model:**"
      ],
      "metadata": {
        "id": "NCFOdW0ABKp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (train_images.shape[1], train_images.shape[2], train_images.shape[3])\n",
        "model = unet_model(input_shape)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(train_labels.shape)\n",
        "print(val_labels.shape)\n",
        "print(train_images.shape)\n",
        "print(val_images.shape)\n",
        "num_classes = 2  #2 classes: roof and non-roof\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "print(f\"train_labels_cat shape after conversion: {train_labels_cat.shape}\")\n",
        "print(f\"val_labels_cat shape after conversion: {val_labels_cat.shape}\")\n",
        "history = model.fit(train_images, train_labels_cat, epochs=20, batch_size=5, validation_data=(val_images, val_labels_cat))\n",
        "model.save('/content/drive/MyDrive/save')\n",
        "#val_loss, val_accuracy = model.evaluate(val_images, val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk92AIkiLa0v",
        "outputId": "40ce181e-fbe9-4b80-a5ee-267873aec890"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 256, 256)\n",
            "(5, 256, 256)\n",
            "(20, 256, 256, 3, 1)\n",
            "(5, 256, 256, 3, 1)\n",
            "train_labels_cat shape after conversion: (20, 256, 256, 2)\n",
            "val_labels_cat shape after conversion: (5, 256, 256, 2)\n",
            "Epoch 1/20\n",
            "4/4 [==============================] - 10s 448ms/step - loss: 0.8045 - accuracy: 0.6946 - val_loss: 0.4644 - val_accuracy: 0.8615\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 1s 309ms/step - loss: 0.5324 - accuracy: 0.8527 - val_loss: 0.5109 - val_accuracy: 0.8615\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 1s 307ms/step - loss: 0.5180 - accuracy: 0.8527 - val_loss: 0.9435 - val_accuracy: 0.8615\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 1s 306ms/step - loss: 0.6154 - accuracy: 0.8527 - val_loss: 0.4640 - val_accuracy: 0.8615\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 1s 327ms/step - loss: 0.4885 - accuracy: 0.8527 - val_loss: 0.4462 - val_accuracy: 0.8615\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 1s 312ms/step - loss: 0.4711 - accuracy: 0.8527 - val_loss: 0.4322 - val_accuracy: 0.8615\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 1s 305ms/step - loss: 0.4496 - accuracy: 0.8527 - val_loss: 0.4083 - val_accuracy: 0.8615\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 1s 332ms/step - loss: 0.4206 - accuracy: 0.8527 - val_loss: 0.3866 - val_accuracy: 0.8615\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 1s 360ms/step - loss: 0.3957 - accuracy: 0.8527 - val_loss: 0.3730 - val_accuracy: 0.8615\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 1s 344ms/step - loss: 0.3785 - accuracy: 0.8527 - val_loss: 0.3612 - val_accuracy: 0.8615\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 1s 364ms/step - loss: 0.3643 - accuracy: 0.8527 - val_loss: 0.3568 - val_accuracy: 0.8615\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 1s 330ms/step - loss: 0.3586 - accuracy: 0.8527 - val_loss: 0.3689 - val_accuracy: 0.8615\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 1s 334ms/step - loss: 0.3560 - accuracy: 0.8527 - val_loss: 0.3470 - val_accuracy: 0.8615\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 1s 312ms/step - loss: 0.3485 - accuracy: 0.8527 - val_loss: 0.3454 - val_accuracy: 0.8619\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 1s 311ms/step - loss: 0.3379 - accuracy: 0.8640 - val_loss: 0.4142 - val_accuracy: 0.8241\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 1s 310ms/step - loss: 0.3975 - accuracy: 0.8588 - val_loss: 0.3367 - val_accuracy: 0.8615\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 1s 310ms/step - loss: 0.3619 - accuracy: 0.8527 - val_loss: 0.4008 - val_accuracy: 0.8615\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 1s 336ms/step - loss: 0.3649 - accuracy: 0.8527 - val_loss: 0.3404 - val_accuracy: 0.8615\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 1s 355ms/step - loss: 0.3502 - accuracy: 0.8527 - val_loss: 0.3373 - val_accuracy: 0.8615\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 1s 406ms/step - loss: 0.3349 - accuracy: 0.8527 - val_loss: 0.3403 - val_accuracy: 0.8615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predictions and Visualization of them :**"
      ],
      "metadata": {
        "id": "JEyeCdv3BP5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_images)\n",
        "predictions_path = '/content/drive/MyDrive/predictions'\n",
        "os.makedirs(predictions_path, exist_ok=True)\n",
        "\n",
        "for i, pred in enumerate(predictions):\n",
        "    pred_mask = np.argmax(pred, axis=-1)\n",
        "    pred_mask = (pred.squeeze() * 255).astype(np.uint8)\n",
        "    cv.imwrite(os.path.join(predictions_path, f'prediction_{i}.png'), pred_mask)\n",
        "import matplotlib.pyplot as plt\n",
        "def display_predictions(test_images, predictions, num=5):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i in range(num):\n",
        "        plt.subplot(num, 3, i*3 + 1)\n",
        "        plt.imshow(test_images[i].squeeze(), cmap='gray')\n",
        "        plt.title(\"Test Image\")\n",
        "\n",
        "        plt.subplot(num, 3, i*3 + 2)\n",
        "        plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
        "        plt.title(\"Prediction\")\n",
        "\n",
        "        plt.subplot(num, 3, i*3 + 3)\n",
        "        plt.imshow(test_images[i].squeeze(), cmap='gray')\n",
        "        plt.imshow(predictions[i].squeeze(), cmap='jet', alpha=0.5)\n",
        "        plt.title(\"Overlay\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display predictions for the first 5 test images\n",
        "display_predictions(test_images, predictions, num=5)"
      ],
      "metadata": {
        "id": "56JFb4ab7S1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRASH:**"
      ],
      "metadata": {
        "id": "tUF_51mTQcG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    filenames = sorted(os.listdir(folder))\n",
        "    for filename in filenames:\n",
        "        img = cv.imread(os.path.join(folder, filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return images\n",
        "\n",
        "train_image = np.array(load_images_from_folder(training_image_path))\n",
        "label = np.array(load_images_from_folder(label_path))\n",
        "test_image = np.array(load_images_from_folder(test_image_path))\n",
        "\n",
        "#normalization\n",
        "train_image = np.array(train_image) / 255.0\n",
        "label = np.array(label) / 255.0\n",
        "test_image = np.array(test_image) / 255.0"
      ],
      "metadata": {
        "id": "hoE0EhyeCyXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#binarize the labels\n",
        "labels = np.where(labels > 0.5, 1, 0)\n",
        "\n",
        "#needed for keras -> need to look it up again\n",
        "train_images = train_images[..., np.newaxis]\n",
        "labels = labels[..., np.newaxis]\n",
        "test_images = test_images[..., np.newaxis]\n",
        "\n",
        "# split training and validation -> nicht ganz verstanden\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3zIpoN_dC9Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential model define (simple):"
      ],
      "metadata": {
        "id": "EmXz3QZw6qjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "#Encodes\n",
        "model.add(InputLayer(input_shape=aligned_train_images.shape[1:]))\n",
        "print(aligned_train_images.shape)\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# Decoder\n",
        "model.add(Conv2DTranspose(512, (2, 2), strides=(2, 2), activation='relu', padding='same'))\n",
        "\n",
        "model.add(Conv2DTranspose(256, (2, 2), strides=(2, 2), activation='relu', padding='same'))\n",
        "\n",
        "model.add(Conv2DTranspose(128, (2, 2), strides=(2, 2), activation='relu', padding='same'))\n",
        "\n",
        "model.add(Conv2DTranspose(64, (2, 2), strides=(2, 2), activation='relu', padding='same'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Conv2D(1, (1, 1), activation='sigmoid'))"
      ],
      "metadata": {
        "id": "rXPHSz1Y6o5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}